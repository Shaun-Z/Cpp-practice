{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Prepares the input for multi-head attention by reshaping it.\n",
    "    This module reshapes the input tensor to have an additional dimension for the number of heads.\n",
    "    \n",
    "    Args:\n",
    "    - d_model (int): The dimension of the model.\n",
    "    - heads (int): The number of attention heads.\n",
    "    - d_k (int): The dimension of each head.\n",
    "    - bias (bool): Whether to include a bias term in the linear transformation.\n",
    "\n",
    "    Example:\n",
    "    >>> prepare = PrepareForMultiHeadAttention(d_model=512, heads=8, d_k=64, bias=True)\n",
    "    >>> input_tensor = torch.randn(10, 20, 512)  # (batch_size, seq_len, d_model)\n",
    "    >>> output_tensor = prepare(input_tensor)\n",
    "    output_tensor.shape  # Should be (10, 8, 20, 64)\n",
    "    '''\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        # Linear layer for linear transformation\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        # Number of heads\n",
    "        self.heads = heads\n",
    "        # Dimension of each head\n",
    "        self.d_k = d_k\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass to reshape the input tensor for multi-head attention.\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        Returns:\n",
    "        - torch.Tensor: Reshaped tensor of shape (batch_size, heads, seq_len, d_k).\n",
    "        '''\n",
    "\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        # Apply linear transformation\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Reshape to (batch_size, heads, seq_len, d_k)\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "\n",
    "        # Output has shape (batch_size, heads, seq_len, d_k) or (batch_size, seq_len, heads, d_k)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850be4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
